{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJE7+9/dY2Q2ioJ10epdbA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avaneeshp23/GenAIhackathon/blob/hackathon/genaihackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKIqxyTHZ_VB",
        "outputId": "d97288cb-0176-4bb7-d296-7efc3fad2826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/avaneeshp23/GenAIhackathon.git"
      ],
      "metadata": {
        "id": "A0C9rafxm7JB",
        "outputId": "f6080f03-eb2b-4ad3-f83f-27f7da6536f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GenAIhackathon'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 12 (delta 0), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 153.59 KiB | 814.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "Xtv_flxgfqtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "eDm5jaumaMA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=os.environ['GROQ_API_KEY'])"
      ],
      "metadata": {
        "id": "yRLqLEUdeFNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CPf50bXip9e",
        "outputId": "01f371a4-e399-4985-a21e-53117e7d26c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I need to explain the importance of fast language models. Hmm, where do I start? I know that language models are a big part of AI, especially in NLP. But what makes them important? Well, speed is a key factor. Maybe I should think about why speed matters in different applications.\n",
            "\n",
            "First, real-time applications come to mind. If a model is too slow, it can't be used in real-time chatbots or translation services. People expect quick responses, so speed is crucial there. Then, scalability. If a company wants to handle millions of users, a slow model would be a bottleneck. Fast models can handle more requests without crashing or slowing down the system.\n",
            "\n",
            "Efficiency is another point. Training models takes a lot of resources. Faster models would use less compute time and energy, which is better for the environment and saves money. Also, during inference, faster models can run on smaller devices, which is great for edge computing where resources are limited.\n",
            "\n",
            "User experience is important too. If a model takes too long to respond, users get frustrated. Faster models keep the interaction smooth and seamless. In areas like healthcare or customer service, quick responses can make a big difference in satisfaction and effectiveness.\n",
            "\n",
            "Research and development also benefit from fast models. If you can train and test models quickly, you can iterate faster. This leads to more innovation and quicker progress in the field. Plus, fast models can process more data in less time, which can lead to better insights and more accurate results.\n",
            "\n",
            "Accessibility is another angle. If models are fast and efficient, they can be deployed on mobile devices and other consumer products, making advanced NLP features available to more people. This can bridge the gap between different regions and languages, promoting inclusivity.\n",
            "\n",
            "Competitiveness in business is another factor. Companies that can deploy fast models can offer better services, respond faster to customers, and gain an edge over competitors. This can lead to better customer retention and acquisition.\n",
            "\n",
            "Ethical considerations come into play too. Fast models can detect and mitigate harmful content more quickly, reducing the spread of misinformation. They can also monitor and improve fairness in AI systems, ensuring they're unbiased.\n",
            "\n",
            "Looking into the future, fast models will enable more advanced AI systems. As models get bigger, efficiency becomes key. Fast models can handle larger tasks and more complex queries without performance issues.\n",
            "\n",
            "Putting this all together, the importance of fast language models spans across various areas from user experience and efficiency to business competitiveness and ethical considerations. They enable real-time applications, improve scalability, reduce costs, enhance user satisfaction, accelerate innovation, increase accessibility, provide a competitive edge, support ethical AI, and pave the way for future advancements in AI.\n",
            "\n",
            "I think that covers most of the key points. Maybe I should organize these thoughts into a coherent explanation, making sure each point is clear and connected logically.\n",
            "</think>\n",
            "\n",
            "Fast language models play a pivotal role in advancing various aspects of technology and user experience. Here's a structured explanation of their importance:\n",
            "\n",
            "1. **Real-Time Applications**: Fast language models are essential for applications requiring immediate responses, such as chatbots and translation services. They ensure interactions are swift and seamless, meeting user expectations for instant feedback.\n",
            "\n",
            "2. **Scalability**: In environments handling millions of users, fast models prevent bottlenecks, allowing systems to manage high request volumes efficiently without degradation in performance.\n",
            "\n",
            "3. **Efficiency**: These models reduce computational resources and energy consumption during training and inference. This not only lowers costs but also supports environmental sustainability by minimizing the carbon footprint.\n",
            "\n",
            "4. **User Experience**: Quick response times enhance user satisfaction, particularly in critical sectors like healthcare and customer service, where delays can impact effectiveness and satisfaction.\n",
            "\n",
            "5. **Research and Development**: Faster training and inference accelerate the iteration process, fostering innovation and progress in AI. They enable rapid processing of large datasets, leading to better insights and accuracy.\n",
            "\n",
            "6. **Accessibility**: Efficient models can be deployed on mobile devices, broadening access to advanced NLP features and promoting inclusivity across different regions and languages.\n",
            "\n",
            "7. **Competitiveness**: Businesses leveraging fast models can offer superior services, respond swiftly to customers, and gain a competitive edge, leading to improved customer retention and acquisition.\n",
            "\n",
            "8. **Ethical Considerations**: They facilitate quick detection of harmful content and biases, enhancing the safety and fairness of AI systems.\n",
            "\n",
            "9. **Future Advancements**: Fast models pave the way for more advanced AI systems, efficiently handling larger tasks and complex queries as models grow in size and capability.\n",
            "\n",
            "In summary, fast language models are crucial for enhancing user experience, driving efficiency, enabling scalability, and supporting ethical AI practices, while also fostering innovation and competitiveness across industries.\n"
          ]
        }
      ]
    }
  ]
}